{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":572434,"sourceType":"datasetVersion","datasetId":276801}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predictive Maintenance using enhanced CMAPSS dataset\n\n## objective:\nsystem that predicts when the engine is starting to wear out so that you can fix it early\nwe don't want a simplistic model, as real engines gradually degrade\n\nproject goals: \n- detect stage of degradation\n- predicts how long it'll stay in that shape before it gets worse\n- calculate a risk score\n  \n\napproach:\n- **Clustering** to identify 5 health stages\n- **Classification** to estimate current health\n- **Regression** to predict time to next failure\n- **Risk scoring** to trigger intelligent maintenance alerts\n\n\n## dataset:\nNASA CMAPSS: [Link](https://www.kaggle.com/datasets/behrad3d/nasa-cmaps)","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:41:14.891199Z","iopub.execute_input":"2025-05-08T13:41:14.891622Z","iopub.status.idle":"2025-05-08T13:41:14.897843Z","shell.execute_reply.started":"2025-05-08T13:41:14.891597Z","shell.execute_reply":"2025-05-08T13:41:14.896908Z"}}},{"cell_type":"markdown","source":"## Phase 1 : data-preprocessing","metadata":{"execution":{"iopub.status.busy":"2025-05-08T13:43:18.364046Z","iopub.execute_input":"2025-05-08T13:43:18.364339Z","iopub.status.idle":"2025-05-08T13:43:18.368222Z","shell.execute_reply.started":"2025-05-08T13:43:18.364320Z","shell.execute_reply":"2025-05-08T13:43:18.367402Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:36:50.636518Z","iopub.status.idle":"2025-05-08T17:36:50.636850Z","shell.execute_reply.started":"2025-05-08T17:36:50.636705Z","shell.execute_reply":"2025-05-08T17:36:50.636717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Original CMAPSS column names\ncolumns = ['unit_number','time_in_cycles','setting_1','setting_2','TRA','T2','T24','T30','T50','P2','P15','P30','Nf',\n           'Nc','epr','Ps30','phi','NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n\n# Chosen degradation-sensitive features\nselected_features = ['T24', 'T30', 'T50', 'P15', 'P30', 'Nf', 'Nc', \n                     'Ps30', 'phi', 'NRf', 'NRc', 'BPR', 'htBleed', 'W31', 'W32']\n\ntrain_data_processed = []\n\n# Preprocess each dataset (FD001–FD004)\nfor idx in range(1, 5):\n    train_data = pd.read_csv(f\"/kaggle/input/nasa-cmaps/CMaps/train_FD00{idx}.txt\", sep=\" \", header=None)\n    \n    # Drop trailing blank columns\n    train_data.drop(columns=[26, 27], inplace=True)\n    \n    # Assign meaningful column names\n    train_data.columns = columns\n    \n    # Keep only required columns\n    train_data = train_data[['unit_number', 'time_in_cycles'] + selected_features]\n    \n    # Calculate normalized RUL (life ratio)\n    train_data['RUL'] = train_data['time_in_cycles'] / train_data.groupby('unit_number')['time_in_cycles'].transform('max')\n    \n    train_data_processed.append(train_data)\n    print(f\"Processed train_FD00{idx}.txt, number of rows: {train_data.shape[0]}\")\n\n# Combine all datasets\nall_data = pd.concat(train_data_processed, ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data_processed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Normalize sensor features\nscaler = StandardScaler()\nall_data[selected_features] = scaler.fit_transform(all_data[selected_features])\n\n# ✅ Save scaler for future use\nwith open(\"scaler.pkl\", \"wb\") as f:\n    pickle.dump(scaler, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Save preprocessed dataset\nall_data.to_pickle(\"preprocessed_FD001_004.pkl\")\n\nprint(\"✅ Preprocessing complete. Saved to preprocessed_FD001_004.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 2 : clustering & degradation stage labeling","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\n\ndf = pd.read_pickle(\"preprocessed_FD001_004.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_features = ['T24', 'T30', 'T50', 'P15', 'P30', 'Nf', 'Nc', \n                     'Ps30', 'phi', 'NRf', 'NRc', 'BPR', 'htBleed', 'W31', 'W32']\n\nkmeans = KMeans(n_clusters=5, random_state=42, n_init=42)\ndf['raw_cluster'] = kmeans.fit_predict(df[selected_features])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cluster_rul_means = df.groupby('raw_cluster')['RUL'].mean().sort_values(ascending=False)\ncluster_to_stage = {cluster: i for i, cluster in enumerate(cluster_rul_means.index)}\ndf['degradation_stage'] = df['raw_cluster'].map(cluster_to_stage)\ndf.drop(columns=['raw_cluster'], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stage_counts = df['degradation_stage'].value_counts().sort_index()\nprint(\"Degradation Stage Distribution:\")\nprint(stage_counts)\n\nsns.barplot(x=stage_counts.index, y=cluster_rul_means.values)\nplt.xlabel(\"Degradation Stage\")\nplt.ylabel(\"Average RUL\")\nplt.title(\"Cluster-derived Degradation Stages\")\nplt.grid(True)\nplt.show()\n\ndf.to_pickle(\"clustered_FD001_004.pkl\")\nprint(\"✅ Clustering complete. Saved to clustered_FD001_004.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\ndf = pd.read_pickle(\"clustered_FD001_004.pkl\")\n\n# Features\nselected_features = ['T24', 'T30', 'T50', 'P15', 'P30', 'Nf', 'Nc', \n                     'Ps30', 'phi', 'NRf', 'NRc', 'BPR', 'htBleed', 'W31', 'W32']\n\n# Subsample: 5000 rows max (tunable)\ndf_sample = df.sample(n=5000, random_state=42).copy()\n\n# Run Agglomerative Clustering on the sample\nagglo = AgglomerativeClustering(n_clusters=5)\ndf_sample['agglo_cluster'] = agglo.fit_predict(df_sample[selected_features])\n\n# Map clusters to stages based on average RUL\nagglo_rul_means = df_sample.groupby('agglo_cluster')['RUL'].mean().sort_values(ascending=False)\nagglo_stage_map = {cluster: i for i, cluster in enumerate(agglo_rul_means.index)}\ndf_sample['agglo_stage'] = df_sample['agglo_cluster'].map(agglo_stage_map)\n\n# Show result\nprint(\"Agglomerative Stage Distribution (Sampled):\")\nprint(df_sample['agglo_stage'].value_counts().sort_index())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 3 : classification and regression","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# Train KNN on sampled clusters to generalize to full dataset\nX_knn_train = df_sample[selected_features]\ny_knn_train = df_sample['agglo_stage']\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_knn_train, y_knn_train)\n\n# Predict on full dataset\ndf['agglo_stage'] = knn.predict(df[selected_features])\n\nprint(\"Predicted Agglomerative Stage Distribution (Full Data):\")\nprint(df['agglo_stage'].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\nX_class = df[selected_features]\ny_class = df['degradation_stage']\n\nX_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_class, y_class, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Random Forest\nclf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\nclf.fit(X_train_c, y_train_c)\ny_pred_rf = clf.predict(X_test_c)\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test_c, y_pred_rf))\nprint(classification_report(y_test_c, y_pred_rf))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Logistic Regression\nlogreg = LogisticRegression(max_iter=1000, class_weight='balanced')\nlogreg.fit(X_train_c, y_train_c)\ny_pred_log = logreg.predict(X_test_c)\nprint(\"Logistic Regression Accuracy:\", accuracy_score(y_test_c, y_pred_log))\nprint(classification_report(y_test_c, y_pred_log))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SVC\nsvc = SVC(probability=True, class_weight='balanced')\nsvc.fit(X_train_c, y_train_c)\ny_pred_svc = svc.predict(X_test_c)\nprint(\"SVC Accuracy:\", accuracy_score(y_test_c, y_pred_svc))\nprint(classification_report(y_test_c, y_pred_svc))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_cycles_to_next_stage(df):\n    \"\"\"Computes how many cycles until the next degradation stage.\"\"\"\n    cycles_to_next = []\n\n    for unit in df['unit_number'].unique():\n        unit_df = df[df['unit_number'] == unit].reset_index(drop=True)\n        n = len(unit_df)\n\n        for i in range(n):\n            current_stage = unit_df.loc[i, 'degradation_stage']\n            future = unit_df.iloc[i+1:]\n            next_stage = future[future['degradation_stage'] > current_stage]\n\n            if not next_stage.empty:\n                delta = next_stage.index[0] - i\n            else:\n                delta = 0  # already at final stage\n\n            cycles_to_next.append(delta)\n\n    return cycles_to_next\n\n# Apply it\ndf['cycles_to_next_stage'] = compute_cycles_to_next_stage(df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Filter for valid regression rows\nregression_df = df[df['cycles_to_next_stage'] > 0].copy()\nX_regression = regression_df[selected_features]\ny_regression = regression_df['cycles_to_next_stage']\n\nX_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_regression, y_regression, test_size=0.2, random_state=42)\n\nmodels = {\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n    'Ridge': Ridge(alpha=1.0),\n    'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.1)\n}\n\nresults = {}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.base import clone\nimport time\n\nresults = {}\n\nfor name, base_model in models.items():\n    print(f\"\\nTraining {name}…\", flush=True)\n    \n    # Clone so we don't mutate the original in the dict\n    model = clone(base_model)\n    \n    # If the model supports parallelism, switch it on\n    if hasattr(model, \"n_jobs\"):\n        model.set_params(n_jobs=-1)\n    \n    t0 = time.time()\n    model.fit(X_train_r, y_train_r)\n    y_pred = model.predict(X_test_r)\n    elapsed = time.time() - t0\n    \n    # Metrics\n    mse  = mean_squared_error(y_test_r, y_pred)\n    rmse = np.sqrt(mse)\n    mae  = mean_absolute_error(y_test_r, y_pred)\n    r2   = r2_score(y_test_r, y_pred)\n    \n    results[name] = {\n        \"model\": model, \"predictions\": y_pred,\n        \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R²\": r2,\n        \"fit_time_s\": elapsed\n    }\n    \n    print(f\"{name}: RMSE={rmse:.2f} | MAE={mae:.2f} | R²={r2:.2f} | \"\n          f\"time={elapsed:.1f}s\", flush=True)\n\nbest_model_name = max(results, key=lambda k: results[k][\"R²\"])\nbest_predictions = results[best_model_name][\"predictions\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:03:56.123207Z","iopub.execute_input":"2025-05-08T17:03:56.123598Z"}},"outputs":[{"name":"stdout","text":"\nTraining Random Forest…\nRandom Forest: RMSE=75.19 | MAE=53.97 | R²=0.02 | time=93.3s\n\nTraining Ridge…\nRidge: RMSE=74.86 | MAE=53.85 | R²=0.03 | time=0.1s\n\nTraining SVR…\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4 : risk Score Computation + maintenance Alert Logic","metadata":{}},{"cell_type":"code","source":"# Risk Score Calculation\ndef calculate_risk_score(df, classifier, regressor):\n    X = df[selected_features]\n    stage_probs = classifier.predict_proba(X)\n    df['failure_prob'] = stage_probs[:, 4]\n    df['time_to_next_stage'] = regressor.predict(X)\n    epsilon = 1e-6\n    df['raw_risk_product'] = df['failure_prob'] * df['time_to_next_stage']\n    df['urgency_score'] = df['failure_prob'] / (df['time_to_next_stage'] + epsilon)\n    for col in ['raw_risk_product', 'urgency_score']:\n        min_val, max_val = df[col].min(), df[col].max()\n        df[f'normalized_{col}'] = (df[col] - min_val) / (max_val - min_val + epsilon)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:02:46.296201Z","iopub.execute_input":"2025-05-08T17:02:46.296626Z","iopub.status.idle":"2025-05-08T17:02:46.303750Z","shell.execute_reply.started":"2025-05-08T17:02:46.296598Z","shell.execute_reply":"2025-05-08T17:02:46.302712Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Optimal Threshold Finder\nfrom sklearn.metrics import precision_recall_curve, auc\n\ndef find_optimal_threshold(df, true_label_col):\n    df['imminent_failure'] = (df[true_label_col] <= 30).astype(int)\n    precision, recall, thresholds = precision_recall_curve(df['imminent_failure'], df['normalized_urgency_score'])\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)\n    best_idx = np.argmax(f1_scores)\n    best_threshold = thresholds[best_idx]\n    return best_threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:02:48.429664Z","iopub.execute_input":"2025-05-08T17:02:48.430571Z","iopub.status.idle":"2025-05-08T17:02:48.436536Z","shell.execute_reply.started":"2025-05-08T17:02:48.430534Z","shell.execute_reply":"2025-05-08T17:02:48.435499Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Alert System\ndef generate_alerts(df, threshold=0.7):\n    df['alert_product'] = df['normalized_raw_risk_product'] > threshold\n    df['alert_urgency'] = df['normalized_urgency_score'] > threshold\n    df['alert_combined'] = df['alert_product'] & df['alert_urgency']\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:02:49.002880Z","iopub.execute_input":"2025-05-08T17:02:49.003209Z","iopub.status.idle":"2025-05-08T17:02:49.008545Z","shell.execute_reply.started":"2025-05-08T17:02:49.003170Z","shell.execute_reply":"2025-05-08T17:02:49.007460Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# Identify the best regression model by R²\nbest_model_name = max(results, key=lambda k: results[k]['R²'])\nbest_predictions = results[best_model_name]['predictions']\n\n# Apply end-to-end\nclassifier = clf\nregressor = results[best_model_name]['model']\n\ndf_with_risk = calculate_risk_score(df, classifier, regressor)\noptimal_threshold = find_optimal_threshold(df_with_risk, 'time_to_next_stage')\ndf_with_alerts = generate_alerts(df_with_risk, threshold=optimal_threshold)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:03:50.218418Z","iopub.execute_input":"2025-05-08T17:03:50.219096Z","iopub.status.idle":"2025-05-08T17:03:51.375948Z","shell.execute_reply.started":"2025-05-08T17:03:50.219065Z","shell.execute_reply":"2025-05-08T17:03:51.374992Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"## Some Visualisations : ","metadata":{}},{"cell_type":"markdown","source":"## Sensor Trends by Degradation Stage","metadata":{}},{"cell_type":"code","source":"def visualize_sensor_trends():\n    \"\"\"\n    Visualize how key sensors change across different degradation stages\n    \"\"\"\n    # Load the processed dataset\n    df = pd.read_pickle(\"clustered_FD001_004.pkl\")\n    \n    # Select a random engine unit for visualization\n    unit = np.random.choice(df['unit_number'].unique())\n    unit_data = df[df['unit_number'] == unit].sort_values('time_in_cycles')\n    \n    # Select key sensors that typically show degradation patterns\n    key_sensors = ['T24', 'T30', 'P30', 'Nf', 'Ps30', 'phi', 'NRf', 'BPR', 'W31']\n    \n    # Create a multi-panel plot\n    fig, axes = plt.subplots(len(key_sensors), 1, figsize=(12, 20), sharex=True)\n    fig.suptitle(f'Sensor Readings Over Time for Engine Unit {unit}', fontsize=16)\n    \n    # Set a colormap for degradation stages\n    cmap = plt.cm.get_cmap('RdYlGn_r', 5)\n    stage_colors = [cmap(i) for i in range(5)]\n    \n    # Plot each sensor\n    for i, sensor in enumerate(key_sensors):\n        ax = axes[i]\n        scatter = ax.scatter(\n            unit_data['time_in_cycles'], \n            unit_data[sensor],\n            c=unit_data['degradation_stage'], \n            cmap=plt.cm.get_cmap('RdYlGn_r', 5),\n            alpha=0.7,\n            s=30\n        )\n        \n        # Add vertical lines where degradation stage changes\n        stage_changes = unit_data.loc[unit_data['degradation_stage'].diff() != 0, 'time_in_cycles']\n        for change in stage_changes:\n            ax.axvline(x=change, color='gray', linestyle='--', alpha=0.5)\n        \n        ax.set_ylabel(sensor, fontsize=10)\n        ax.grid(True, alpha=0.3)\n        \n        # Add trend line\n        z = np.polyfit(unit_data['time_in_cycles'], unit_data[sensor], 1)\n        p = np.poly1d(z)\n        ax.plot(unit_data['time_in_cycles'], p(unit_data['time_in_cycles']), \n                \"r--\", alpha=0.8, linewidth=1)\n    \n    # Add a legend for degradation stages\n    cbar = fig.colorbar(scatter, ax=axes.ravel().tolist(), pad=0.01, aspect=40)\n    cbar.set_label('Degradation Stage', fontsize=12)\n    cbar.set_ticks(np.arange(0.5, 5.5))\n    cbar.set_ticklabels(['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3', 'Stage 4'])\n    \n    axes[-1].set_xlabel('Operating Cycles', fontsize=12)\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.95)\n    plt.show()\n    \n    return fig\n\n# Call the function\nvisualize_sensor_trends()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:36:50.637907Z","iopub.status.idle":"2025-05-08T17:36:50.638281Z","shell.execute_reply.started":"2025-05-08T17:36:50.638094Z","shell.execute_reply":"2025-05-08T17:36:50.638110Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## risk score dashboard","metadata":{}},{"cell_type":"code","source":"def visualize_risk_dashboard():\n    \"\"\"\n    Create a risk score dashboard showing multiple engines and their degradation\n    \"\"\"\n    # Load the data with risk scores\n    df_with_alerts = pd.read_pickle(\"clustered_FD001_004.pkl\")\n    \n    # Compute risk scores if they don't exist\n    if 'normalized_urgency_score' not in df_with_alerts.columns:\n        # If risk scores don't exist, create mock data for visualization\n        print(\"Risk scores not found, creating mock data for visualization\")\n        df_with_alerts['normalized_urgency_score'] = df_with_alerts['RUL'] * -1 + 1  # Inverse of RUL\n        df_with_alerts['normalized_urgency_score'] = (df_with_alerts['normalized_urgency_score'] - \n                                                     df_with_alerts['normalized_urgency_score'].min()) / \\\n                                                    (df_with_alerts['normalized_urgency_score'].max() - \n                                                     df_with_alerts['normalized_urgency_score'].min())\n        df_with_alerts['alert_urgency'] = df_with_alerts['normalized_urgency_score'] > 0.7\n    \n    # Select a few engine units for the dashboard\n    n_engines = 6\n    selected_units = np.random.choice(df_with_alerts['unit_number'].unique(), \n                                     size=min(n_engines, len(df_with_alerts['unit_number'].unique())), \n                                     replace=False)\n    \n    # Create plotly subplot figure\n    fig = make_subplots(rows=n_engines, cols=1, \n                        subplot_titles=[f\"Engine Unit {unit}\" for unit in selected_units],\n                        shared_xaxes=True,\n                        vertical_spacing=0.02)\n    \n    # Color scale for risk score\n    colorscale = [[0, 'green'], [0.5, 'yellow'], [0.7, 'orange'], [1.0, 'red']]\n    \n    # Add traces for each engine\n    for i, unit in enumerate(selected_units):\n        unit_df = df_with_alerts[df_with_alerts['unit_number'] == unit].sort_values('time_in_cycles')\n        \n        # Main trace with risk score as color\n        fig.add_trace(\n            go.Scatter(\n                x=unit_df['time_in_cycles'],\n                y=unit_df['normalized_urgency_score'],\n                mode='lines+markers',\n                marker=dict(\n                    size=8,\n                    color=unit_df['normalized_urgency_score'],\n                    colorscale=colorscale,\n                    showscale=True if i == 0 else False,  # Only show colorbar for first subplot\n                    colorbar=dict(title=\"Risk Score\") if i == 0 else None\n                ),\n                line=dict(width=2),\n                name=f\"Unit {unit}\"\n            ),\n            row=i+1, col=1\n        )\n        \n        # Add alert markers\n        if 'alert_urgency' in unit_df.columns:\n            alerts = unit_df[unit_df['alert_urgency']]\n            if not alerts.empty:\n                fig.add_trace(\n                    go.Scatter(\n                        x=alerts['time_in_cycles'],\n                        y=alerts['normalized_urgency_score'],\n                        mode='markers',\n                        marker=dict(\n                            symbol='star',\n                            size=12,\n                            color='red',\n                            line=dict(width=1, color='black')\n                        ),\n                        name=\"Alert\"\n                    ),\n                    row=i+1, col=1\n                )\n        \n        # Add threshold line\n        if 'alert_urgency' in unit_df.columns:\n            threshold = unit_df[unit_df['alert_urgency']]['normalized_urgency_score'].min() \\\n                        if not unit_df[unit_df['alert_urgency']].empty else 0.7\n            fig.add_shape(\n                type=\"line\",\n                x0=min(unit_df['time_in_cycles']),\n                y0=threshold,\n                x1=max(unit_df['time_in_cycles']),\n                y1=threshold,\n                line=dict(color=\"red\", width=1, dash=\"dash\"),\n                row=i+1, col=1\n            )\n    \n    # Update layout\n    fig.update_layout(\n        title_text=\"Engine Risk Score Dashboard\",\n        showlegend=False,\n        height=200 * n_engines,\n        width=1000,\n        margin=dict(l=50, r=50, t=50, b=50)\n    )\n    \n    # Update y-axes\n    for i in range(1, n_engines + 1):\n        fig.update_yaxes(title_text=\"Risk Score\", range=[0, 1], row=i, col=1)\n    \n    # Update bottom x-axis\n    fig.update_xaxes(title_text=\"Operating Cycles\", row=n_engines, col=1)\n    \n    fig.show()\n    return fig\n\n# Call the function\nvisualize_risk_dashboard()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}